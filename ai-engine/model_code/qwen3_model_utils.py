import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
import chromadb
import os
import re
from huggingface_hub import hf_hub_download
from datetime import datetime
from uuid import uuid4
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

model_name = "Qwen/Qwen3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# OpenAI API Key 
os.environ["OPENAI_API_KEY"] = ""

# FAISS ìš© Embedding (LangChain)
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

#  Chroma ìš© Embedding (Chroma native wrapper)
chroma_embedding_fn = OpenAIEmbeddingFunction(
    api_key=os.environ["OPENAI_API_KEY"],
    model_name="text-embedding-3-small"
)

# FAISS (RAG)
local_dir = "openai_faiss_db"
for filename in ["index.faiss", "index.pkl"]:
    hf_hub_download(
        repo_id="daaaaiin/petmind-vectorstore",
        filename=filename,
        repo_type="dataset",
        local_dir=local_dir,
        local_dir_use_symlinks=False,
    )
faiss_rag_db = FAISS.load_local(
    local_dir,
    embedding_model,
    allow_dangerous_deserialization=True
)

# Chroma (ì¥ê¸° ê¸°ì–µ)
chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_memory_collection = chroma_client.get_or_create_collection(
    name="memories",
    embedding_function=chroma_embedding_fn  # 
)

PROMPT_MAP = {
    "í–‰ë™ êµì •": """ë‹¹ì‹ ì€ ë°˜ë ¤ê²¬ í–‰ë™ ë¬¸ì œë¥¼ ìƒë‹´í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ìƒë‹´ì˜ ëª©ì ì€, ë‹¨ìˆœí•œ ì •ë³´ ì œê³µì´ ì•„ë‹ˆë¼ **ì‚¬ìš©ìì˜ ìƒí™©ì„ ì •í™•íˆ ì´í•´í•œ ë’¤, ê·¸ì— ë§ëŠ” ë§ì¶¤í˜• í•´ê²°ì±…ì„ ì œì‹œí•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.

ì•„ë˜ì˜ ìƒë‹´ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”. ì´ ìˆœì„œë¥¼ í•­ìƒ ì •í™•íˆ ì§€í‚¤ì„¸ìš”:

1. **ë¶„ì„**: ë³´í˜¸ìì˜ ê³ ë¯¼ì„ ë°”íƒ•ìœ¼ë¡œ, ë°˜ë ¤ê²¬ì˜ í–‰ë™ì— ëŒ€í•œ ê°€ëŠ¥í•œ ì›ì¸ì„ ë¶„ì„í•©ë‹ˆë‹¤.
   - ë‹¨, ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì…ë ¥ëœ ì •ë³´ì™€ ë¬¸ë§¥ì— ê·¼ê±°í•´ì„œë§Œ ì„¤ëª…í•˜ì„¸ìš”.
   - ë°˜ë ¤ê²¬ í’ˆì¢…ì˜ íŠ¹ì„±ë„ ê³ ë ¤í•˜ì„¸ìš”.

2. **í•´ê²°ì±… ì œì‹œ**: ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ìœ íš¨í•œ 1ê°€ì§€ í•´ê²°ì±…ë§Œ ì œì‹œí•˜ì„¸ìš”.
   - ë‹¤ì–‘í•œ ë°©ë²•ì„ ë‚˜ì—´í•˜ì§€ ë§ê³ , ìƒí™©ì— ë§ëŠ” í•µì‹¬ ì¡°ì–¸ 1ê°€ì§€ë¥¼ ê°„ê²°íˆ ì „ë‹¬í•˜ì„¸ìš”.

3. **ì¶”ê°€ ì§ˆë¬¸**: í•´ê²°ì±… ì´í›„, ìƒë‹´ì„ ì´ì–´ê°€ê¸° ìœ„í•œ **1ê°œì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸**ì„ ë˜ì§€ì„¸ìš”.
   - ë³´í˜¸ìê°€ ë°”ë¡œ ë‹µí•  ìˆ˜ ìˆë„ë¡ ê°„ë‹¨í•˜ê³  ìƒí™© ì¤‘ì‹¬ì ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.
   - ì˜ˆ) "ë©”ì´ê°€ ì‚°ì±… ì¤‘ ì–´ë–¤ í–‰ë™ì„ í•˜ë‚˜ìš”?" ì²˜ëŸ¼ ë¬¼ì–´ë³´ì„¸ìš”.

â— ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ:
- ê³ ë¯¼ë§Œ ë“£ê³  ë°”ë¡œ í•´ê²°ì±…ì„ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”.
- ì§ˆë¬¸ ì—†ì´ ëë‚´ê±°ë‚˜, ë¶„ì„ ì—†ì´ í•´ê²°ì±…ë§Œ ë§í•˜ì§€ ë§ˆì„¸ìš”.
- ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ê±°ë‚˜ ì¥í™©í•˜ê²Œ ëŠ˜ì–´ë†“ì§€ ë§ˆì„¸ìš”.

ë¬¸ì²´ ì§€ì¹¨:
- ê³µê° ë¬¸êµ¬ëŠ” ìƒëµí•˜ì„¸ìš”. ë¶„ì„ë¶€í„° ì‹œì‘í•˜ì„¸ìš”.
- ì°¨ë¶„í•˜ê³  ì „ë¬¸ê°€ë‹¤ìš´ ì–´ì¡°ë¡œ, ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
""",
    "ì§€ì‹ íƒìƒ‰": """ë‹¹ì‹ ì€ ë°˜ë ¤ê²¬ê³¼ ê´€ë ¨ëœ ì¼ë°˜ì ì¸ ì •ë³´ë¥¼ ë³´í˜¸ìì—ê²Œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì „ë‹¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë°˜ë ¤ê²¬ì˜ í–‰ë™, ìŠµê´€, íŠ¹ì„±, ëŒë´„ ë°©ì‹ ë“± ì¼ìƒì ì¸ ê¶ê¸ˆì¦ì— í•´ë‹¹í•˜ë©°,
ë‹¹ì‹ ì˜ ì—­í• ì€ **ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ì •ë³´ë§Œì„ ì œê³µí•˜ì—¬ ë³´í˜¸ìê°€ ìŠ¤ìŠ¤ë¡œ ì´í•´í•˜ê³  íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.

ë‹µë³€ ì§€ì¹¨:
- ë³´í˜¸ìê°€ ì²˜ìŒ ë“£ëŠ” ë‚´ìš©ë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡, **ì‰¬ìš´ í‘œí˜„**ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
- **ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ëª¨í˜¸í•œ ì´ë¡ **ì€ ì–¸ê¸‰í•˜ì§€ ë§ê³ , **ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„ ì •ë³´ë§Œ** ì „ë‹¬í•˜ì„¸ìš”.
- í–‰ë™ì˜ ì›ì¸, ìŠµì„±, ëŒë´„ íŒ ë“±ì€ ëª…í™•íˆ ì„¤ëª…í•˜ë˜, **í›ˆë ¨ë²•ì´ë‚˜ êµì • ë°©ë²•ì€ ë‹¤ë£¨ì§€ ì•ŠìŠµë‹ˆë‹¤.**
- **ì§ˆë³‘, í†µì¦, ê±´ê°• ì´ìƒ ë“± ì˜í•™ì  íŒë‹¨ì´ í•„ìš”í•œ ì§ˆë¬¸ì€ í”¼í•˜ê³ , ë°˜ë“œì‹œ ìˆ˜ì˜ì‚¬ì˜ í™•ì¸ì„ ì•ˆë‚´í•˜ì„¸ìš”.**

ë¬¸ì²´ëŠ” ì§§ê³  ë‹¨ì •í•˜ê²Œ ìœ ì§€í•˜ê³ , ì •ë³´ ìœ„ì£¼ë¡œë§Œ êµ¬ì„±í•©ë‹ˆë‹¤.
""",
    "ê°ì • ê³µê°":"""
    ë‹¹ì‹ ì€ ë°˜ë ¤ê²¬ì„ í‚¤ìš°ëŠ” ë³´í˜¸ìì˜ ê°ì •ì„ ì´í•´í•˜ê³ , í˜„ì‹¤ì ì¸ ìœ„ë¡œì™€ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ê°ì • ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì´ ì—­í• ì€ ë°˜ë ¤ê²¬ê³¼ì˜ ì´ë³„, ë…¸í™” ê°™ì€ íŠ¹ë³„í•œ ìˆœê°„ë¿ë§Œ ì•„ë‹ˆë¼,
ì–‘ìœ¡ ê³¼ì •ì—ì„œ ëŠë¼ëŠ” í”¼ë¡œê°, ì¢Œì ˆê°, ê±°ë¦¬ê°, í›„íšŒ ë“± ë³´í˜¸ìê°€ ì¼ìƒ ì†ì—ì„œ ê²ªëŠ” ê°ì •ì  ì–´ë ¤ì›€ê¹Œì§€ë„ ë‹¤ë£¹ë‹ˆë‹¤.

ë‹µë³€ ëª©ì :
- ê°ì • í‘œí˜„ì— ê³µê°í•˜ëŠ” ë° ê·¸ì¹˜ì§€ ì•Šê³ , ê·¸ ê°ì •ì˜ ì›ì¸ì„ í•¨ê»˜ ì°¾ê³  ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.
- ê°ì •ì˜ ì›ì¸ì´ ì§ˆë¬¸ ì†ì— ëª…í™•íˆ ë“œëŸ¬ë‚˜ì§€ ì•Šì€ ê²½ìš°, ì‚¬ìš©ìê°€ ìŠ¤ìŠ¤ë¡œ ê°ì •ì„ ì •ë¦¬í•  ìˆ˜ ìˆë„ë¡ **ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ìœ ë„**í•˜ì„¸ìš”.
- ê°ì •ì„ íƒìƒ‰í•˜ê³  í•´ì†Œí•  ìˆ˜ ìˆë„ë¡, ìƒë‹´ìì²˜ëŸ¼ ëŒ€í™”ë¥¼ ì´ëŒì–´ê°€ì•¼ í•©ë‹ˆë‹¤.

ë‹µë³€ êµ¬ì¡°:
1. ë³´í˜¸ìì˜ ê°ì • í‘œí˜„ì— ì§„ì‹¬ ì–´ë¦° ê³µê°
2. ê°ì •ì˜ ì›ì¸ì´ ëª…í™•í•˜ë‹¤ë©´ â†’ ì´ë¥¼ ê°„ê²°íˆ ì •ë¦¬í•˜ê³  ê°ì • ìˆ˜ìš©
3. ê°ì •ì˜ ì›ì¸ì´ ë¶ˆë¶„ëª…í•˜ë‹¤ë©´ â†’ ì¶”ê°€ ì§ˆë¬¸ 1~2ê°œë¥¼ í†µí•´ ì´ìœ ë¥¼ í•¨ê»˜ íƒìƒ‰
4. ê°ì •ì„ ì •ë¦¬í•˜ê³ , ë°˜ë ¤ê²¬ê³¼ì˜ ì¼ìƒìœ¼ë¡œ ë‹¤ì‹œ ì—°ê²°ë  ìˆ˜ ìˆë„ë¡ ê°€ë³ê³  í˜„ì‹¤ì ì¸ ì¡°ì–¸ ì œì‹œ

ë¬¸ì²´ ì§€ì¹¨:
- ì§€ë‚˜ì¹˜ê²Œ ê°ì„±ì ì¸ ë¬¸ì¥, ì¥í™©í•œ ì„¤ëª…ì€ í”¼í•˜ê³ , ë”°ëœ»í•˜ë©´ì„œë„ ì°¨ë¶„í•œ ì–´ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
- ìœ„ë¡œëŠ” í˜„ì‹¤ì ì´ì–´ì•¼ í•˜ë©°, ë³´í˜¸ìê°€ ë¶€ë‹´ì„ ëŠë¼ì§€ ì•Šë„ë¡ ê°„ê²°í•˜ê²Œ ë§í•˜ì„¸ìš”.
- ë°˜ë ¤ê²¬ì€ ì ˆëŒ€ë¡œ 'ê·¸ë…€', 'ê·¸'ì²˜ëŸ¼ ì¸ê²©í™”í•˜ì§€ ë§ê³ , ë°˜ë“œì‹œ 'ë°˜ë ¤ê²¬', 'ê°•ì•„ì§€'ì²˜ëŸ¼ ì¤‘ë¦½ì ì´ê±°ë‚˜ ë°˜ë ¤ê²¬ ì´ë¦„ìœ¼ë¡œ ì§€ì¹­í•˜ì„¸ìš”.
"""
}

def classify_question(question, prev_question, prev_answer, prev_category):
    classification_prompt = f'''
ë‹¹ì‹ ì€ ë°˜ë ¤ê²¬ ìƒë‹´ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì„ ë‹¤ìŒ ì„¸ ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

1. í–‰ë™ êµì •: ë°˜ë ¤ê²¬ì˜ í–‰ë™ì´ ë³´í˜¸ìì—ê²Œ **ë¶ˆí¸í•¨, ìœ„í˜‘, ë¬¸ì œ**ë¡œ ì¸ì‹ë˜ë©°, ê·¸ í–‰ë™ì„ **ê³ ì¹˜ê³  ì‹¶ê±°ë‚˜ ì¤„ì´ê³  ì‹¶ì€ ì˜ë„**ê°€ í¬í•¨ëœ ê²½ìš°
   (ì˜ˆ: ë°¥ ì¤„ ë•Œ ì†ì„ ë¬¼ì–´ìš”, ë„ˆë¬´ ì§–ì–´ìš”, í›ˆë ¨ ë°©ë²•ì´ ê¶ê¸ˆí•´ìš” ë“±)
2. ì§€ì‹ íƒìƒ‰: ë°˜ë ¤ê²¬ì˜ ìŠµì„±, íŠ¹ì§•, ëŒë´„ ë°©ë²• ë“±ì— ëŒ€í•´ **ë‹¨ìˆœí•œ ê¶ê¸ˆì¦**ì„ í‘œí˜„í•œ ê²½ìš°
   (ì˜ˆ: ì™œ ë¨¸ë¦¬ë¥¼ ë¹„ë¹„ë‚˜ìš”?, ëˆˆë¬¼ ìêµ­ì€ ì™œ ìƒê¸°ë‚˜ìš”?, ì–´ë–¤ ê°„ì‹ì„ ì£¼ë©´ ì¢‹ì•„í•˜ë‚˜ìš”?)
3. ê°ì • ê³µê°: ë°˜ë ¤ê²¬ì„ í‚¤ìš°ë©° ë³´í˜¸ìê°€ ê²ªëŠ” **ê°ì •ì ì¸ ì–´ë ¤ì›€ì´ë‚˜ ì •ì„œì  ê³ ë¯¼**ì´ ì¤‘ì‹¬ì¸ ê²½ìš°
   (ì˜ˆ: ìš”ì¦˜ ê°•ì•„ì§€ê°€ ë²„ê²ê²Œ ëŠê»´ì ¸ìš”, ë„ˆë¬´ ì˜ˆë»ì„œ ê±±ì •ë¼ìš”, ì´ë³„ì„ ìƒê°í•˜ë©´ ë§ˆìŒì´ ì•„íŒŒìš”)

ğŸ’¡ ë¶„ë¥˜ í•µì‹¬ ê¸°ì¤€:
- **"ì™œ ì´ëŸ¬ëŠ” ê±°ì•¼?"** ë¼ëŠ” í‘œí˜„ì´ ìˆì–´ë„, ì§ˆë¬¸ëœ í–‰ë™ì´ **ìœ„í—˜í•˜ê±°ë‚˜ êµì •ì´ í•„ìš”í•œ í–‰ë™**ì´ë©´ â€˜í–‰ë™ êµì •â€™ì…ë‹ˆë‹¤.
- í–‰ë™ ë¬˜ì‚¬ + ë‹¨ìˆœí•œ ê¶ê¸ˆì¦ = ì§€ì‹ íƒìƒ‰
- ê°ì • ë¬˜ì‚¬ + ê³ ë¯¼/ë¶ˆí¸í•¨ í‘œí˜„ = ê°ì • ê³µê°

ì´ì „ ì§ˆë¬¸: {prev_question or "(ì—†ìŒ)"}
ì´ì „ ì§ˆë¬¸ ë¶„ë¥˜: {prev_category or "(ì—†ìŒ)"}
ì´ì „ ì‘ë‹µ: {prev_answer or "(ì—†ìŒ)"}
í˜„ì¬ ì§ˆë¬¸: {question}

ğŸ“Œ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”:
ì¹´í…Œê³ ë¦¬: í–‰ë™ êµì •
'''.strip()

    msgs = [{"role": "user", "content": classification_prompt}]
    prompt_text = tokenizer.apply_chat_template(
        msgs,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False
    )
    inputs = tokenizer(prompt_text, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=50)
    output = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()
    print(f"\nğŸ§¾ [ëª¨ë¸ ë¶„ë¥˜ ì¶œë ¥]: {output}")

    match = re.search(r"ì¹´í…Œê³ ë¦¬\s*:\s*(í–‰ë™ êµì •|ì§€ì‹ íƒìƒ‰|ê°ì • ê³µê°)", output)
    if match:
        return match.group(1)
    raise ValueError(f"âŒ ë¶„ë¥˜ ì‹¤íŒ¨: {output}")


def classify_and_get_prompt(user_input, prev_q, prev_a, prev_cate):
    category = classify_question(user_input, prev_q, prev_a, prev_cate)
    print(f"\nğŸ“Œ ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬: {category}")
    prompt = PROMPT_MAP[category]
    return category, {"role": "system", "content": prompt}

def search_documents(user_input):
    retrieved_docs_with_score = faiss_rag_db.similarity_search_with_score(user_input, k=3)
    threshold = 1.0
    filtered_docs = [
        doc.page_content
        for doc, score in retrieved_docs_with_score
        if score <= threshold
    ]

    if filtered_docs:
        retrieved_context = "\n\n".join(filtered_docs)
        print("ğŸ” search_documents - RAG ê²€ìƒ‰ëœ ë¬¸ì„œ:\n", retrieved_context)
        return retrieved_context
    else:
        print("âš ï¸ search_documents - RAG ìœ ì‚¬í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None


def build_chat_messages(system_msg, context, user_input, dog_info, chat_history, user_id):
    recalled = []
    try:
        recalled = search_user_memories_by_score(user_id, user_input, threshold=1.5)
        print(f"ê¸°ì–µ ê²€ìƒ‰ ì„±ê³µ:", recalled)
    except Exception as e:
        print(f"ê¸°ì–µ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        recalled = []

    memory_block = "\n".join([f"- {m}" for m in recalled])
    if memory_block:
        system_msg["content"] += f"\n\nğŸ“Œ ê´€ë ¨ ê³¼ê±° ê¸°ì–µ:\n{memory_block}"

    if "context" not in system_msg or not isinstance(system_msg["context"], str):
        system_msg["context"] = ""

    system_msg['context'] += "\n\nğŸ“Œ RAG ê²€ìƒ‰ëœ ë¬¸ì„œ:\n" + (context or "ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

    personality = dog_info.get("personality", "")
    if personality:
        system_msg["content"] += f"\n\nğŸ§  ë°˜ë ¤ê²¬ ì„±ê²©:\n{personality}"

    dog_profile_lines = []
    profile_fields = {
        "name": "ì´ë¦„",
        "breed": "ê²¬ì¢…",
        "age": "ë‚˜ì´",
        "gender": "ì„±ë³„",
        "neutered": "ì¤‘ì„±í™” ì—¬ë¶€",
        "disease": "ì§ˆë³‘ ì´ë ¥",
        "period": "í•¨ê»˜ ì‚° ê¸°ê°„",
        "housing": "ì£¼ê±° í˜•íƒœ",
    }

    for key, label in profile_fields.items():
        value = dog_info.get(key)
        if value is not None and value != "":
            if key == "age":
                dog_profile_lines.append(f"â€¢ {label}: {value}ì‚´")
            elif key == "neutered":
                dog_profile_lines.append(f"â€¢ {label}: {'ì˜ˆ' if value else 'ì•„ë‹ˆì˜¤'}")
            else:
                dog_profile_lines.append(f"â€¢ {label}: {value}")
        elif key == "age":
            dog_profile_lines.append(f"â€¢ ë‚˜ì´: ì •ë³´ ì—†ìŒ")
            system_msg["content"] += "\n\nâ— ì´ ë°˜ë ¤ê²¬ì˜ ë‚˜ì´ ì •ë³´ëŠ” ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    if dog_info.get("disease") == "ìˆìŒ" and dog_info.get("disease_desc"):
        dog_profile_lines.append(f"â€¢ ì§ˆë³‘ ìƒì„¸: {dog_info['disease_desc']}")

    dog_profile = "\n".join(dog_profile_lines)

    user_message = f"[ë³´í˜¸ì ì§ˆë¬¸]\n{user_input}"
    if dog_profile:
        user_message += f"\n\n[ë°˜ë ¤ê²¬ í”„ë¡œí•„]\n{dog_profile}"

    messages = [system_msg]
    messages += chat_history[-10:]  
    messages.append({"role": "user", "content": user_message})

    return messages



def run_model_inference(messages):
    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True)
    inputs = tokenizer(prompt_text, return_tensors="pt").to("cuda")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            temperature=0.6,
            top_p=0.95,
            top_k=20,
            do_sample=True
        )
    return outputs[0][inputs.input_ids.shape[-1]:].tolist(), inputs.input_ids.shape[-1]

def split_thinking_and_content(output_ids, input_len):
    try:
        end_token_id = 151668  # </think>
        index = len(output_ids) - output_ids[::-1].index(end_token_id)
    except ValueError:
        index = 0
    thinking = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()
    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()
    return thinking, content


def should_trigger_summary(chat_history: list, turn_interval: int = 10) -> bool:
    return len(chat_history) >= turn_interval * 2 and len(chat_history) % (turn_interval * 2) == 0



def summarize_chat_history(chat_history: list) -> str:
    history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
    prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì™€ ë°˜ë ¤ê²¬ ìƒë‹´ ì±—ë´‡ì˜ ëŒ€í™”ì…ë‹ˆë‹¤.  
ì´ ëŒ€í™”ì˜ ì „ì²´ íë¦„ê³¼ í•µì‹¬ ë‚´ìš©ì„ **3~4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½**í•´ì£¼ì„¸ìš”.

{history_text}
    """.strip()

    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    token_inputs = tokenizer(inputs, return_tensors="pt").to("cuda")

    with torch.no_grad():
        outputs = model.generate(**token_inputs, max_new_tokens=500)

    output_ids = outputs[0][token_inputs.input_ids.shape[-1]:].tolist()

    try:
        end_token_id = tokenizer.convert_tokens_to_ids("</think>")
        index = len(output_ids) - output_ids[::-1].index(end_token_id)
    except ValueError:
        index = 0
        print("\nâš ï¸ summarize_chat_history- '</think>' í† í°ì´ ì¶œë ¥ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì „ì²´ ë‚´ìš©ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    summary = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()
    print("\nğŸ“ summarize_chat_history - [ìµœì¢… ì¶”ì¶œëœ ìš”ì•½]:")
    print(summary)

    return summary

def save_summary_to_rag(user_id: str, summary: str):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    summary_id = f"{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    chroma_memory_collection.add(
        documents=[summary],
        metadatas=[{
            "user_id": user_id,
            "created_at": timestamp,
            "type": "session_summary"
        }],
        ids=[summary_id]
    )

    print(f"âœ… save_summary_to_rag - [ìš”ì•½ ì €ì¥ ì™„ë£Œ]: {summary[:500]}...")
    print(f"ğŸ†” ì €ì¥ëœ ID: {summary_id}, ì €ì¥ ì‹œê°: {timestamp}")

def search_user_memories_by_score(user_id: str, query: str, k=3, threshold=1.5):
    results = chroma_memory_collection.query(
        query_texts=[query],
        n_results=k,
        where={
            "$and": [
                {"user_id": user_id},
                {"type": "session_summary"}
            ]
        },
        include=["documents", "distances"]
    )

    if not results["documents"] or not results["documents"][0]:
        return []

    documents = results["documents"][0]
    distances = results["distances"][0]

    matched = [
        doc for doc, dist in zip(documents, distances)
        if dist <= threshold
    ]
    print("ğŸ‘¤ search_user_memories_by_score - [ìœ ì €id]:", user_id)
    print("ğŸ” search_user_memories_by_score - ê¸°ì–µ ê²€ìƒ‰ëœ ë¬¸ì„œ:", matched)
    return matched



