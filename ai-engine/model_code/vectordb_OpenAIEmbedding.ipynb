{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28735,
     "status": "ok",
     "timestamp": 1747603100207,
     "user": {
      "displayName": "오",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "_kWSUM_BSjLl",
    "outputId": "1fe18631-c5cd-40a3-fd13-40f6538913f7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install -q --upgrade langchain\n",
    "!pip install -q --upgrade langchain-openai\n",
    "!pip install -q --upgrade langchain_community\n",
    "!pip install -q transformers\n",
    "!pip install -q faiss-gpu\n",
    "!pip install -q pandas\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuZJtCNnqvAU"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y numpy\n",
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 974
    },
    "executionInfo": {
     "elapsed": 25647,
     "status": "ok",
     "timestamp": 1747603125868,
     "user": {
      "displayName": "오",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "psoyehatQd5b",
    "outputId": "70431bd1-d7b9-40c2-b8dd-fa2698b9aa11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.2.6\n",
      "Uninstalling numpy-2.2.6:\n",
      "  Successfully uninstalled numpy-2.2.6\n",
      "Found existing installation: pandas 2.2.3\n",
      "Uninstalling pandas-2.2.3:\n",
      "  Successfully uninstalled pandas-2.2.3\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e25c7e475cae4ecb9f254493e51815e6",
       "pip_warning": {
        "packages": [
         "dateutil",
         "six"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip uninstall numpy pandas -y\n",
    "!pip install numpy pandas --upgrade --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE9OQj9ipksf"
   },
   "source": [
    "## 벡터 db 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXq0upI1SLc8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import math\n",
    "\n",
    "# 1. CSV 불러와서 Document로 변환\n",
    "def load_documents(path=\"merged_rag_data.csv\"):\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    docs = []\n",
    "    for i, row in df.iterrows():\n",
    "        title = str(row.get(\"title\", \"\")).strip()\n",
    "        content = str(row.get(\"content\", \"\")).strip()\n",
    "\n",
    "        if content:\n",
    "            full_text = f\"{title} - {content}\" if title else content\n",
    "            doc = Document(\n",
    "                page_content=full_text,\n",
    "                metadata={\n",
    "                    \"source\": f\"doc_{i}\",\n",
    "                    \"title\": title\n",
    "                }\n",
    "            )\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"총 문서 수: {len(docs)}\")\n",
    "    return docs\n",
    "\n",
    "# 2. 문서 분할 (chunking)\n",
    "def split_documents(documents):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# 3. 임베딩 & 벡터 DB 생성 및 저장\n",
    "def build_vector_db(documents, save_path=\"openai_faiss_db\"):\n",
    "    print(\"OpenAI 임베딩 실행 시작\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectordb = FAISS.from_documents(documents, embedding=embeddings)\n",
    "    vectordb.save_local(save_path)\n",
    "    print(f\"벡터 DB 저장 완료\")\n",
    "\n",
    "def build_vector_db(documents, save_path=\"openai_faiss_db\", batch_size=300):\n",
    "    print(\"OpenAI 임베딩 실행 시작\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "    text_embedding_pairs = []\n",
    "\n",
    "    total_batches = math.ceil(len(texts) / batch_size)\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_texts = texts[start:end]\n",
    "        batch_metadatas = metadatas[start:end]\n",
    "\n",
    "        try:\n",
    "            batch_vectors = embeddings.embed_documents(batch_texts)\n",
    "        except Exception as e:\n",
    "            print(f\"임베딩 실패 (배치 {i+1}/{total_batches}): {e}\")\n",
    "            continue\n",
    "\n",
    "        for text, vector, metadata in zip(batch_texts, batch_vectors, batch_metadatas):\n",
    "            text_embedding_pairs.append((text, vector, metadata))\n",
    "\n",
    "        print(f\"배치 {i+1}/{total_batches} 임베딩 완료\")\n",
    "\n",
    "    # 직접 벡터 저장\n",
    "    texts, vectors, metadatas = zip(*text_embedding_pairs)\n",
    "    vectordb = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(texts, vectors)),\n",
    "        embedding=embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    vectordb.save_local(save_path)\n",
    "    print(f\"벡터 DB 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18434,
     "status": "ok",
     "timestamp": 1747603355042,
     "user": {
      "displayName": "오",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "2IgRuozgRpgi",
    "outputId": "eff88225-1115-49a9-9826-f3b32e2f9681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1838,
     "status": "ok",
     "timestamp": 1747603385226,
     "user": {
      "displayName": "오",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "F2tkYpr0VVuE",
    "outputId": "1fc5f642-b563-4529-a78f-7d4e85753e74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문서 수: 1609\n"
     ]
    }
   ],
   "source": [
    "raw_docs = load_documents(\"/content/drive/MyDrive/merged_rag_data.csv\")\n",
    "split_docs = split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747603389228,
     "user": {
      "displayName": "오",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "ZKMV6UWoVZMk",
    "outputId": "0b5c84cf-eb79-4cc3-ca04-f7142a6bc093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 문서 수: 6500\n",
      "가장 긴 문서 길이 (문자 수): 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"전체 문서 수: {len(split_docs)}\")\n",
    "print(f\"가장 긴 문서 길이 (문자 수): {max(len(doc.page_content) for doc in split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "HQt73SJhU6JT",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "# api_key = userdata.get(\"\")\n",
    "# if api_key is None:\n",
    "#     raise ValueError(\"OPENAI_API_KEY 설정X\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123812,
     "status": "ok",
     "timestamp": 1747603745480,
     "user": {
      "displayName": "오",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "ZVXqTMQPStgy",
    "outputId": "b68a8ca5-9c91-4158-ad2d-1b35b4827f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI 임베딩 실행 시작\n",
      "배치 1/22 임베딩 완료\n",
      "배치 2/22 임베딩 완료\n",
      "배치 3/22 임베딩 완료\n",
      "배치 4/22 임베딩 완료\n",
      "배치 5/22 임베딩 완료\n",
      "배치 6/22 임베딩 완료\n",
      "배치 7/22 임베딩 완료\n",
      "배치 8/22 임베딩 완료\n",
      "배치 9/22 임베딩 완료\n",
      "배치 10/22 임베딩 완료\n",
      "배치 11/22 임베딩 완료\n",
      "배치 12/22 임베딩 완료\n",
      "배치 13/22 임베딩 완료\n",
      "배치 14/22 임베딩 완료\n",
      "배치 15/22 임베딩 완료\n",
      "배치 16/22 임베딩 완료\n",
      "배치 17/22 임베딩 완료\n",
      "배치 18/22 임베딩 완료\n",
      "배치 19/22 임베딩 완료\n",
      "배치 20/22 임베딩 완료\n",
      "배치 21/22 임베딩 완료\n",
      "배치 22/22 임베딩 완료\n",
      "벡터 DB 저장 완료\n"
     ]
    }
   ],
   "source": [
    "build_vector_db(split_docs, save_path=\"openai_faiss_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99m5zEXwpqGW"
   },
   "source": [
    "## 저장된 벡터 db로 문서 유사도 검색 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKxPc9l_UYzE"
   },
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectordb = FAISS.load_local(\"openai_faiss_db\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R_QMP22pxcQ"
   },
   "outputs": [],
   "source": [
    "query = \"강아지가 계속 짖는 이유가 궁금해요\"\n",
    "results = vectordb.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n📄 결과 {i+1}\")\n",
    "    print(\"내용:\", doc.page_content)\n",
    "    print(\"메타데이터:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REDvMjg1rRyI"
   },
   "source": [
    "## 모델 추론 과정에 RAG구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUI9VIwPwDvg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import PeftModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZOtUKq0wHZO"
   },
   "outputs": [],
   "source": [
    "# === 1. 모델 & 토크나이저 로드 ===\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkHBQo8kwJng"
   },
   "outputs": [],
   "source": [
    "# === 2. 시스템 프롬프트 설정 ===\n",
    "system_prompt = \"\"\"당신은 반려견 행동 문제를 상담해주는 전문가입니다.\n",
    "\n",
    "상담의 목적은, 단순한 정보 제공이 아니라 **사용자의 상황을 정확히 이해한 뒤, 그에 맞는 맞춤형 해결책을 제시하는 것**입니다.\n",
    "\n",
    "아래의 상담 구조를 반드시 따르세요:\n",
    "\n",
    "1. 사용자가 고민을 입력하면, 그 고민의 원인을 '추측'하거나 '일반화'하지 말고, **반드시 추가 질문을 통해 정보를 더 수집**하세요.\n",
    "2. **반려견의 품종 정보를 고려하여** 행동 특성, 기질, 환경 민감도를 분석에 반영하세요.\n",
    "3. 질문은 1개로 짧게, **사용자가 답하기 쉽도록 구체적이고 상황 중심적으로** 만들어야 합니다.\n",
    "4. 추가 질문이 1-2번 이루어졌으면, **해결책을 1가지로 요약해서 제시**하세요.\n",
    "   (여러 해결책을 나열하거나 조건 없이 모두 설명하지 마세요.)\n",
    "5. 모든 답변은 **공감 → 질문 또는 분석 → 해결책 제시**의 흐름을 따라야 합니다.\n",
    "\n",
    "- 상담의 시작은 항상 보호자의 감정을 공감하는 문장으로 시작하세요.\n",
    "- 문장의 시작에는 다음 형식을 사용하세요:\n",
    "  **\"안녕하세요! (반려견 이름) 보호자님! (반려견 이름)의 (고민 내용) 때문에 고민이 많으시겠어요.\"**\n",
    "\n",
    "❗절대 하지 말아야 할 것:\n",
    "- 고민 입력만으로 바로 해결책을 나열하지 마세요.\n",
    "- 질문 없이 바로 솔루션을 제시하지 마세요.\n",
    "- 같은 내용을 반복하거나 불필요하게 장황하게 설명하지 마세요.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmXzLhb8qvAg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcFvtJViwL2_"
   },
   "outputs": [],
   "source": [
    "# === 3. FAISS 벡터 DB 로딩 ===\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectordb = FAISS.load_local(\"openai_faiss_db\", embedding_model, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loK0FjQgqvAg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVivMWEJp5t7"
   },
   "outputs": [],
   "source": [
    "# === 4. 대화 루프 시작 ===\n",
    "print(\"🐾 반려견 행동 상담 챗봇입니다. '완료'를 입력하면 종료됩니다.\")\n",
    "dog_breed = input(\"\\n🐶 반려견 종을 입력하세요: \").strip()\n",
    "dog_name = input(\"\\n🐶 반려견의 이름을 입력하세요: \").strip()\n",
    "while True:\n",
    "    user_input = input(\"\\n🧑 사용자 고민: \").strip()\n",
    "    if \"완료\" in user_input:\n",
    "        print(\"\\n✅ 대화를 종료합니다.\")\n",
    "        break\n",
    "\n",
    "   # === ✅ RAG: 관련 문서 검색\n",
    "    retrieved_docs = vectordb.similarity_search(user_input, k=3)\n",
    "\n",
    "    print(\"\\n📄 [검색된 문서 요약]\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        # print(f\"\\n🔎 문서 {i+1}:\\n{doc.page_content[:300]}...\")  # 필요 시 300자 이상도 출력 가능\n",
    "        if doc.metadata:\n",
    "            print(f\"   ⤷ 출처: {doc.metadata}\")\n",
    "\n",
    "    retrieved_context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # === ✅ RAG context 포함한 사용자 메시지 구성\n",
    "    user_message = f\"관련 정보:\\n{retrieved_context}\\n\\n사용자 반려견 정보:\\n견종: {dog_breed}\\n이름: {dog_name}\\n\\n질문:\\n{user_input}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    # === 5. ChatML 템플릿 적용\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            top_k=20,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    output_ids = outputs[0][inputs.input_ids.shape[-1]:].tolist()\n",
    "\n",
    "    # === 6. 사고모드 </think> 분리\n",
    "    try:\n",
    "        end_token_id = 151668  # </think>\n",
    "        index = len(output_ids) - output_ids[::-1].index(end_token_id)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    thinking = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # === 7. 응답 출력 및 메시지 추가\n",
    "    # print(f\"\\n🧠 [thinking]: {thinking}\")\n",
    "    print(f\"🤖 [assistant]: {content}\")\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXMzOR1vsXgq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGztWv2Or9nm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
