{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28735,
     "status": "ok",
     "timestamp": 1747603100207,
     "user": {
      "displayName": "ì˜¤",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "_kWSUM_BSjLl",
    "outputId": "1fe18631-c5cd-40a3-fd13-40f6538913f7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install -q --upgrade langchain\n",
    "!pip install -q --upgrade langchain-openai\n",
    "!pip install -q --upgrade langchain_community\n",
    "!pip install -q transformers\n",
    "!pip install -q faiss-gpu\n",
    "!pip install -q pandas\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DuZJtCNnqvAU"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall -y numpy\n",
    "# !pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 974
    },
    "executionInfo": {
     "elapsed": 25647,
     "status": "ok",
     "timestamp": 1747603125868,
     "user": {
      "displayName": "ì˜¤",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "psoyehatQd5b",
    "outputId": "70431bd1-d7b9-40c2-b8dd-fa2698b9aa11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.2.6\n",
      "Uninstalling numpy-2.2.6:\n",
      "  Successfully uninstalled numpy-2.2.6\n",
      "Found existing installation: pandas 2.2.3\n",
      "Uninstalling pandas-2.2.3:\n",
      "  Successfully uninstalled pandas-2.2.3\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2025.2\n",
      "    Uninstalling pytz-2025.2:\n",
      "      Successfully uninstalled pytz-2025.2\n",
      "  Attempting uninstall: tzdata\n",
      "    Found existing installation: tzdata 2025.2\n",
      "    Uninstalling tzdata-2025.2:\n",
      "      Successfully uninstalled tzdata-2025.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e25c7e475cae4ecb9f254493e51815e6",
       "pip_warning": {
        "packages": [
         "dateutil",
         "six"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip uninstall numpy pandas -y\n",
    "!pip install numpy pandas --upgrade --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE9OQj9ipksf"
   },
   "source": [
    "## ë²¡í„° db ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXq0upI1SLc8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import math\n",
    "\n",
    "# 1. CSV ë¶ˆëŸ¬ì™€ì„œ Documentë¡œ ë³€í™˜\n",
    "def load_documents(path=\"merged_rag_data.csv\"):\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    docs = []\n",
    "    for i, row in df.iterrows():\n",
    "        title = str(row.get(\"title\", \"\")).strip()\n",
    "        content = str(row.get(\"content\", \"\")).strip()\n",
    "\n",
    "        if content:\n",
    "            full_text = f\"{title} - {content}\" if title else content\n",
    "            doc = Document(\n",
    "                page_content=full_text,\n",
    "                metadata={\n",
    "                    \"source\": f\"doc_{i}\",\n",
    "                    \"title\": title\n",
    "                }\n",
    "            )\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"ì´ ë¬¸ì„œ ìˆ˜: {len(docs)}\")\n",
    "    return docs\n",
    "\n",
    "# 2. ë¬¸ì„œ ë¶„í•  (chunking)\n",
    "def split_documents(documents):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# 3. ì„ë² ë”© & ë²¡í„° DB ìƒì„± ë° ì €ì¥\n",
    "def build_vector_db(documents, save_path=\"openai_faiss_db\"):\n",
    "    print(\"OpenAI ì„ë² ë”© ì‹¤í–‰ ì‹œì‘\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectordb = FAISS.from_documents(documents, embedding=embeddings)\n",
    "    vectordb.save_local(save_path)\n",
    "    print(f\"ë²¡í„° DB ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "def build_vector_db(documents, save_path=\"openai_faiss_db\", batch_size=300):\n",
    "    print(\"OpenAI ì„ë² ë”© ì‹¤í–‰ ì‹œì‘\")\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metadatas = [doc.metadata for doc in documents]\n",
    "\n",
    "    text_embedding_pairs = []\n",
    "\n",
    "    total_batches = math.ceil(len(texts) / batch_size)\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_texts = texts[start:end]\n",
    "        batch_metadatas = metadatas[start:end]\n",
    "\n",
    "        try:\n",
    "            batch_vectors = embeddings.embed_documents(batch_texts)\n",
    "        except Exception as e:\n",
    "            print(f\"ì„ë² ë”© ì‹¤íŒ¨ (ë°°ì¹˜ {i+1}/{total_batches}): {e}\")\n",
    "            continue\n",
    "\n",
    "        for text, vector, metadata in zip(batch_texts, batch_vectors, batch_metadatas):\n",
    "            text_embedding_pairs.append((text, vector, metadata))\n",
    "\n",
    "        print(f\"ë°°ì¹˜ {i+1}/{total_batches} ì„ë² ë”© ì™„ë£Œ\")\n",
    "\n",
    "    # ì§ì ‘ ë²¡í„° ì €ì¥\n",
    "    texts, vectors, metadatas = zip(*text_embedding_pairs)\n",
    "    vectordb = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(texts, vectors)),\n",
    "        embedding=embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    vectordb.save_local(save_path)\n",
    "    print(f\"ë²¡í„° DB ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18434,
     "status": "ok",
     "timestamp": 1747603355042,
     "user": {
      "displayName": "ì˜¤",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "2IgRuozgRpgi",
    "outputId": "eff88225-1115-49a9-9826-f3b32e2f9681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1838,
     "status": "ok",
     "timestamp": 1747603385226,
     "user": {
      "displayName": "ì˜¤",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "F2tkYpr0VVuE",
    "outputId": "1fc5f642-b563-4529-a78f-7d4e85753e74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ë¬¸ì„œ ìˆ˜: 1609\n"
     ]
    }
   ],
   "source": [
    "raw_docs = load_documents(\"/content/drive/MyDrive/merged_rag_data.csv\")\n",
    "split_docs = split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747603389228,
     "user": {
      "displayName": "ì˜¤",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "ZKMV6UWoVZMk",
    "outputId": "0b5c84cf-eb79-4cc3-ca04-f7142a6bc093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ë¬¸ì„œ ìˆ˜: 6500\n",
      "ê°€ì¥ ê¸´ ë¬¸ì„œ ê¸¸ì´ (ë¬¸ì ìˆ˜): 512\n"
     ]
    }
   ],
   "source": [
    "print(f\"ì „ì²´ ë¬¸ì„œ ìˆ˜: {len(split_docs)}\")\n",
    "print(f\"ê°€ì¥ ê¸´ ë¬¸ì„œ ê¸¸ì´ (ë¬¸ì ìˆ˜): {max(len(doc.page_content) for doc in split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "HQt73SJhU6JT",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "# api_key = userdata.get(\"\")\n",
    "# if api_key is None:\n",
    "#     raise ValueError(\"OPENAI_API_KEY ì„¤ì •X\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123812,
     "status": "ok",
     "timestamp": 1747603745480,
     "user": {
      "displayName": "ì˜¤",
      "userId": "05931820463777279199"
     },
     "user_tz": -540
    },
    "id": "ZVXqTMQPStgy",
    "outputId": "b68a8ca5-9c91-4158-ad2d-1b35b4827f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI ì„ë² ë”© ì‹¤í–‰ ì‹œì‘\n",
      "ë°°ì¹˜ 1/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 2/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 3/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 4/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 5/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 6/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 7/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 8/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 9/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 10/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 11/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 12/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 13/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 14/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 15/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 16/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 17/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 18/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 19/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 20/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 21/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë°°ì¹˜ 22/22 ì„ë² ë”© ì™„ë£Œ\n",
      "ë²¡í„° DB ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "build_vector_db(split_docs, save_path=\"openai_faiss_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99m5zEXwpqGW"
   },
   "source": [
    "## ì €ì¥ëœ ë²¡í„° dbë¡œ ë¬¸ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKxPc9l_UYzE"
   },
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectordb = FAISS.load_local(\"openai_faiss_db\", embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R_QMP22pxcQ"
   },
   "outputs": [],
   "source": [
    "query = \"ê°•ì•„ì§€ê°€ ê³„ì† ì§–ëŠ” ì´ìœ ê°€ ê¶ê¸ˆí•´ìš”\"\n",
    "results = vectordb.similarity_search(query, k=3)\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nğŸ“„ ê²°ê³¼ {i+1}\")\n",
    "    print(\"ë‚´ìš©:\", doc.page_content)\n",
    "    print(\"ë©”íƒ€ë°ì´í„°:\", doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REDvMjg1rRyI"
   },
   "source": [
    "## ëª¨ë¸ ì¶”ë¡  ê³¼ì •ì— RAGêµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUI9VIwPwDvg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# from peft import PeftModel\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZOtUKq0wHZO"
   },
   "outputs": [],
   "source": [
    "# === 1. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ ===\n",
    "model_name = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkHBQo8kwJng"
   },
   "outputs": [],
   "source": [
    "# === 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì • ===\n",
    "system_prompt = \"\"\"ë‹¹ì‹ ì€ ë°˜ë ¤ê²¬ í–‰ë™ ë¬¸ì œë¥¼ ìƒë‹´í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "ìƒë‹´ì˜ ëª©ì ì€, ë‹¨ìˆœí•œ ì •ë³´ ì œê³µì´ ì•„ë‹ˆë¼ **ì‚¬ìš©ìì˜ ìƒí™©ì„ ì •í™•íˆ ì´í•´í•œ ë’¤, ê·¸ì— ë§ëŠ” ë§ì¶¤í˜• í•´ê²°ì±…ì„ ì œì‹œí•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì•„ë˜ì˜ ìƒë‹´ êµ¬ì¡°ë¥¼ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:\n",
    "\n",
    "1. ì‚¬ìš©ìê°€ ê³ ë¯¼ì„ ì…ë ¥í•˜ë©´, ê·¸ ê³ ë¯¼ì˜ ì›ì¸ì„ 'ì¶”ì¸¡'í•˜ê±°ë‚˜ 'ì¼ë°˜í™”'í•˜ì§€ ë§ê³ , **ë°˜ë“œì‹œ ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ì •ë³´ë¥¼ ë” ìˆ˜ì§‘**í•˜ì„¸ìš”.\n",
    "2. **ë°˜ë ¤ê²¬ì˜ í’ˆì¢… ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬** í–‰ë™ íŠ¹ì„±, ê¸°ì§ˆ, í™˜ê²½ ë¯¼ê°ë„ë¥¼ ë¶„ì„ì— ë°˜ì˜í•˜ì„¸ìš”.\n",
    "3. ì§ˆë¬¸ì€ 1ê°œë¡œ ì§§ê²Œ, **ì‚¬ìš©ìê°€ ë‹µí•˜ê¸° ì‰½ë„ë¡ êµ¬ì²´ì ì´ê³  ìƒí™© ì¤‘ì‹¬ì ìœ¼ë¡œ** ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "4. ì¶”ê°€ ì§ˆë¬¸ì´ 1-2ë²ˆ ì´ë£¨ì–´ì¡Œìœ¼ë©´, **í•´ê²°ì±…ì„ 1ê°€ì§€ë¡œ ìš”ì•½í•´ì„œ ì œì‹œ**í•˜ì„¸ìš”.\n",
    "   (ì—¬ëŸ¬ í•´ê²°ì±…ì„ ë‚˜ì—´í•˜ê±°ë‚˜ ì¡°ê±´ ì—†ì´ ëª¨ë‘ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.)\n",
    "5. ëª¨ë“  ë‹µë³€ì€ **ê³µê° â†’ ì§ˆë¬¸ ë˜ëŠ” ë¶„ì„ â†’ í•´ê²°ì±… ì œì‹œ**ì˜ íë¦„ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ìƒë‹´ì˜ ì‹œì‘ì€ í•­ìƒ ë³´í˜¸ìì˜ ê°ì •ì„ ê³µê°í•˜ëŠ” ë¬¸ì¥ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.\n",
    "- ë¬¸ì¥ì˜ ì‹œì‘ì—ëŠ” ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:\n",
    "  **\"ì•ˆë…•í•˜ì„¸ìš”! (ë°˜ë ¤ê²¬ ì´ë¦„) ë³´í˜¸ìë‹˜! (ë°˜ë ¤ê²¬ ì´ë¦„)ì˜ (ê³ ë¯¼ ë‚´ìš©) ë•Œë¬¸ì— ê³ ë¯¼ì´ ë§ìœ¼ì‹œê² ì–´ìš”.\"**\n",
    "\n",
    "â—ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  ê²ƒ:\n",
    "- ê³ ë¯¼ ì…ë ¥ë§Œìœ¼ë¡œ ë°”ë¡œ í•´ê²°ì±…ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "- ì§ˆë¬¸ ì—†ì´ ë°”ë¡œ ì†”ë£¨ì…˜ì„ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”.\n",
    "- ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ê±°ë‚˜ ë¶ˆí•„ìš”í•˜ê²Œ ì¥í™©í•˜ê²Œ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmXzLhb8qvAg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcFvtJViwL2_"
   },
   "outputs": [],
   "source": [
    "# === 3. FAISS ë²¡í„° DB ë¡œë”© ===\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectordb = FAISS.load_local(\"openai_faiss_db\", embedding_model, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loK0FjQgqvAg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVivMWEJp5t7"
   },
   "outputs": [],
   "source": [
    "# === 4. ëŒ€í™” ë£¨í”„ ì‹œì‘ ===\n",
    "print(\"ğŸ¾ ë°˜ë ¤ê²¬ í–‰ë™ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤. 'ì™„ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\")\n",
    "dog_breed = input(\"\\nğŸ¶ ë°˜ë ¤ê²¬ ì¢…ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "dog_name = input(\"\\nğŸ¶ ë°˜ë ¤ê²¬ì˜ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "while True:\n",
    "    user_input = input(\"\\nğŸ§‘ ì‚¬ìš©ì ê³ ë¯¼: \").strip()\n",
    "    if \"ì™„ë£Œ\" in user_input:\n",
    "        print(\"\\nâœ… ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "   # === âœ… RAG: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰\n",
    "    retrieved_docs = vectordb.similarity_search(user_input, k=3)\n",
    "\n",
    "    print(\"\\nğŸ“„ [ê²€ìƒ‰ëœ ë¬¸ì„œ ìš”ì•½]\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        # print(f\"\\nğŸ” ë¬¸ì„œ {i+1}:\\n{doc.page_content[:300]}...\")  # í•„ìš” ì‹œ 300ì ì´ìƒë„ ì¶œë ¥ ê°€ëŠ¥\n",
    "        if doc.metadata:\n",
    "            print(f\"   â¤· ì¶œì²˜: {doc.metadata}\")\n",
    "\n",
    "    retrieved_context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "    # === âœ… RAG context í¬í•¨í•œ ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±\n",
    "    user_message = f\"ê´€ë ¨ ì •ë³´:\\n{retrieved_context}\\n\\nì‚¬ìš©ì ë°˜ë ¤ê²¬ ì •ë³´:\\nê²¬ì¢…: {dog_breed}\\nì´ë¦„: {dog_name}\\n\\nì§ˆë¬¸:\\n{user_input}\"\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    # === 5. ChatML í…œí”Œë¦¿ ì ìš©\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            temperature=0.6,\n",
    "            top_p=0.95,\n",
    "            top_k=20,\n",
    "            do_sample=True\n",
    "        )\n",
    "\n",
    "    output_ids = outputs[0][inputs.input_ids.shape[-1]:].tolist()\n",
    "\n",
    "    # === 6. ì‚¬ê³ ëª¨ë“œ </think> ë¶„ë¦¬\n",
    "    try:\n",
    "        end_token_id = 151668  # </think>\n",
    "        index = len(output_ids) - output_ids[::-1].index(end_token_id)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    thinking = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip()\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip()\n",
    "\n",
    "    # === 7. ì‘ë‹µ ì¶œë ¥ ë° ë©”ì‹œì§€ ì¶”ê°€\n",
    "    # print(f\"\\nğŸ§  [thinking]: {thinking}\")\n",
    "    print(f\"ğŸ¤– [assistant]: {content}\")\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXMzOR1vsXgq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGztWv2Or9nm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
